---
title: "CARET demo"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

##Installation
```{r eval=F}
install.packages("caret", dependencies = c("Depends", "Suggests"))
```

##Load required libraries
```{r}
library(caret)
library(doMC)
library(corrplot)
```

##Define SVM model
```{r echo=T}
svmRadialE1071 <- list(
  label = "Support Vector Machines with Radial Kernel - e1071",
  library = "e1071",
  type = c("Regression", "Classification"),
  parameters = data.frame(parameter="cost",
                          class="numeric",
                          label="Cost"),
  grid = function (x, y, len = NULL, search = "grid") 
    {
      if (search == "grid") {
        out <- expand.grid(cost = 2^((1:len) - 3))
      }
      else {
        out <- data.frame(cost = 2^runif(len, min = -5, max = 10))
      }
      out
    },
  loop=NULL,
  fit=function (x, y, wts, param, lev, last, classProbs, ...) 
    {
      if (any(names(list(...)) == "probability") | is.numeric(y)) {
        out <- e1071::svm(x = as.matrix(x), y = y, kernel = "radial", 
                          cost = param$cost, ...)
      }
      else {
        out <- e1071::svm(x = as.matrix(x), y = y, kernel = "radial", 
                          cost = param$cost, probability = classProbs, ...)
      }
      out
    },
  predict = function (modelFit, newdata, submodels = NULL) 
    {
      predict(modelFit, newdata)
    },
  prob = function (modelFit, newdata, submodels = NULL) 
    {
      out <- predict(modelFit, newdata, probability = TRUE)
      attr(out, "probabilities")
    },
  predictors = function (x, ...) 
    {
      out <- if (!is.null(x$terms)) 
        predictors.terms(x$terms)
      else x$xNames
      if (is.null(out)) 
        out <- names(attr(x, "scaling")$x.scale$`scaled:center`)
      if (is.null(out)) 
        out <- NA
      out
    },
  tags = c("Kernel Methods", "Support Vector Machines", "Regression", "Classifier", "Robust Methods"),
  levels = function(x) x$levels,
  sort = function(x)
  {
    x[order(x$cost), ]
  }
)

```


##Load data
```{r}
load("data/malaria/malaria.RData")
```

Inspect objects that have been loaded into R session
```{r}
ls()
class(morphology)
dim(morphology)
class(infectionStatus)
summary(as.factor(infectionStatus))
class(stage)
summary(as.factor(stage))
```

##Data splitting
Partition data into a training and test set using the **createDataPartition** function
```{r}
set.seed(42)
trainIndex <- createDataPartition(y=stage, times=1, p=0.7, list=F)
stageTrain <- stage[trainIndex]
morphologyTrain <- morphology[trainIndex,]
stageTest <- stage[-trainIndex]
morphologyTest <- morphology[-trainIndex,]
```

The proportions of the classes in the training and test sets are the same
```{r}
summary(stageTrain)/sum(summary(stageTrain))
summary(stageTest)/sum(summary(stageTest))
```

##Check data quality
CARET provides functions for assessing data quality. The function **nearZeroVar** identifies predictors that have one unique value. It also diagnoses predictors having both of the following characteristics:

* very few unique values relative to the number of samples
* the ratio of the frequency of the most common value to the frequency of the 2nd most common value is large.

Such zero and near zero-variance predictors have a deleterious impact on modelling and may lead to unstable fits.
```{r}
nearZeroVar(morphologyTrain, saveMetrics = T)
```

There are no zero variance or near zero variance predictors in our data set. Here is a synthetic example of a near zero variance predictor:
```{r}
x <- c(rep(1, 960), rep(2, 40))
nearZeroVar(x, saveMetrics=T)
```

Are all predictors on the same scale?
```{r out.width='100%', fig.asp=2, fig.align='center', fig.show='hold', echo=T}
featurePlot(x = morphologyTrain,
            y = stageTrain,
            plot = "box",
            ## Pass in options to bwplot()
            scales = list(y = list(relation="free"),
                          x = list(rot = 90)),
            layout = c(5,5))
```
The variables in this data set are on different scales. In this situation it is important to centre and scale each predictor. A predictor variable is centered by subtracting the mean of the predictor from each value. To scale a predictor variable, each value is divided by its standard deviation. After centring and scaling the predictor variable has a mean of 0 and a standard deviation of 1.

Examine pairwise correlations of predictors to identify redundancy in data set
```{r}
corMat <- cor(morphologyTrain)
corrplot(corMat, order="hclust", tl.cex=1)
```

Find highly correlated predictors
```{r}
highCorr <- findCorrelation(corMat, cutoff=0.75)
length(highCorr)
names(morphologyTrain)[highCorr]
```

The box plots above can be used to examine each predictor for skewness. Alternatively, a density plot can be used
```{r}
featurePlot(x = morphologyTrain[,1:4],
            y = stageTrain,
            plot = "density",
            ## Pass in options to xyplot() to
            ## make it prettier
            scales = list(x = list(relation="free"),
                          y = list(relation="free")),
            adjust = 1.5,
            pch = "|",
            layout = c(2, 2),
            auto.key = list(columns = 2))
```

##Model training and parameter tuning
*k*-NN has only one parameter: the number of nearest neighbours *k*.

We will use repeated cross-validation to find the best value of *k* and we will try 10 different values of *k*. Repeated cross-validation can readily be parallelized to increase speed of execution. All we need to do is create a local cluster - **caret** will then use this cluster to parallelize the cross-validation.

```{r}
registerDoMC(detectCores())
getDoParWorkers()
```

The resampling method is specified using the **trainControl** function. To repeat five-fold cross validation a total of five times we would use:
```{r eval=F}
train_ctrl <- trainControl(method="repeatedcv",
                           number = 5,
                           repeats = 5)
```

To make the analysis reproducible we need to specify the seed for each resampling iteration.
```{r}
set.seed(42)
seeds <- vector(mode = "list", length = 26)
for(i in 1:25) seeds[[i]] <- sample.int(1000, 10)
seeds[[26]] <- sample.int(1000,1)

train_ctrl <- trainControl(method="repeatedcv",
                           number = 5,
                           repeats = 5,
                           seeds = seeds)
```

The **train** function is used to tune a model
```{r}
svmlFit <- train(morphTrain, stageTrain,
                method="svmLinear",
                preProcess = c("center", "scale"),
                #tuneGrid=tuneParam,
                tuneLength=10,
                trControl=train_ctrl)

svmlFit
```

Plot the results of the cross-validation
```{r}
plot(knnFit)
```

The model fit object contains much information
```{r}
names(knnFit)
```

##Try other models
###Random Forest
```{r}
rfFit <- train(morphologyTrain, stageTrain,
                method="rf",
                preProcess = c("center", "scale"),
                #tuneGrid=tuneParam,
                tuneLength=10,
                trControl=train_ctrl)
rfFit

plot(rfFit)
```

###Support Vector Machine
```{r}
svmFit <- train(morphologyTrain, stageTrain,
                method=svmRadialE1071,
                preProcess = c("center", "scale"),
                #tuneGrid=tuneParam,
                tuneLength=10,
                trControl=train_ctrl)

svmFit
```

```{r}
plot(svmFit)
```

##Compare models
Make a list of our models
```{r}
model_list <- list(knn=knnFit,
                   randomForest=rfFit,
                   svm=svmFit)
```

Collect resampling results for each model
```{r}
resamps <- resamples(model_list)
resamps
summary(resamps)
```
```{r}
bwplot(resamps)
```


##Predict test set using our best model
```{r}
test_pred <- predict(rfFit, morphologyTest)
confusionMatrix(test_pred, stageTest)
```



